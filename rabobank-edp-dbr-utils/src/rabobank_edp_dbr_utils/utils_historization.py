from datetime import date
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, sha2, concat_ws, lit, expr, desc, asc_nulls_last
from pyspark.sql.functions import to_timestamp, to_utc_timestamp, current_timestamp
from rabobank_edp_dbr_utils.logger import logging
from rabobank_edp_dbr_utils.util_functions import ValidationUtils
from rabobank_edp_dbr_utils.common.shared_spark_session import get_spark
from rabobank_edp_dbr_utils.logger import logging

utils_logger = logging.getLogger(__name__)

def validate_input_kwargs(**kwargs):
    """
    Validates and merges input keyword arguments with default values.

    This function checks for duplicate keys in the input keyword arguments (case-insensitive),
    assigns default values to missing keys, and returns the merged dictionary.

    Args:
        **kwargs: Arbitrary keyword arguments.

    Returns:
        dict: A dictionary containing the merged keyword arguments with default values.

    Raises:
        ValueError: If duplicate keys are found in the input keyword arguments.
    """
    utils_logger.info("Validating Keyword Arguments")


    #Check if duplicate keys are found (case-insensitive)
    keys_list = list(kwargs.keys())
    upper_case_keys_list = [key.upper() for key in keys_list]

    if len(upper_case_keys_list) != len(set(upper_case_keys_list)):
        utils_logger.error("Keyword Arguments validation failed: Duplicate keys found")
        raise ValueError("Duplicate keys found in the keyword arguments")
        
    #Assigning default values to the kwargs
    default_values ={"primary_key_hash": "EDL_KEY_HASH",
                    "non_primary_key_hash": "EDL_NONKEY_HASH",
                    "edl_load_datetime": "EDL_LOAD_DTS",
                    "edl_act_datetime": "EDL_ACT_DTS",
                    "edl_act_datetime_utc": "EDL_ACT_DTS_UTC",
                    #"edl_change_type": "EDL_CHANGE_TYPE",
                    "edl_valid_from_datetime": "EDL_VALID_FROM_DTS",
                    "edl_valid_from_datetime_utc": "EDL_VALID_FROM_DTS_UTC",
                    "edl_valid_to_datetime": "EDL_VALID_TO_DTS",
                    "edl_valid_to_datetime_utc": "EDL_VALID_TO_DTS_UTC",
                    "edl_active_flag": "EDL_ACTIVE_FLG",
                    "edl_deleted_flag": "EDL_DELETED_FLG",
                    "edl_update_datetime":"EDL_LAST_UPDATE_DTS_UTC",
                    "merge_schema": False
                    }
    # Merge default values with kwargs
    merged_kwargs = {**default_values, **kwargs}
    
    utils_logger.info("Keyword Arguments validated successfully")
    return merged_kwargs

def prepare_insert_update_delete_columns(source_df, partition_name, partition_column, load_type, processing_date, **kwargs):
    """
    Prepare the insert columns for the DataFrame based on the type of load (SCD Type 1 or SCD Type 2).

    Parameters:
        source_df (DataFrame): The source DataFrame.
        partition_name (str): The name of the partition column.
        partition_column (str): The column to be used for partitioning.
        processing_date (str) : The date for which the data is being processed. 

    Keyword Arguments:
        primary_key_hash (str): column name of primary_key_hash (generated by generateEDLHashColumn.py). Example- EDL_KEY_HASH
        non_primary_key_hash (str): column name of non_primary_key_hash (generated by generateEDLHashColumn.py). Example- EDL_NONKEY_HASH
        edl_load_datetime (str): column name to capture load datetime. Example- EDL_LOAD_DTS
        edl_act_datetime (str): column name to capture act datetime. Default - EDL_ACT_DTS
        edl_act_datetime_value (str): name of a functional datetime column in the data to be used in edl_act_datetime column.
        edl_act_datetime_utc (str): column name to capture act datetime UTC. Default - EDL_ACT_DTS_UTC
        edl_act_datetime_utc_value (str): name of a functional datetime UTC column in the data to be used in edl_act_datetime_utc column.
        edl_update_datetime (str): column name to capture last update datetime UTC. Default - EDL_LAST_UPDATE_DTS_UTC        
        edl_valid_from_datetime (str): column name to capture start datetime of a record. Example- EDL_VALID_FROM_DTS
        edl_valid_to_datetime (str): column name to capture end datetime of a record. Example- EDL_VALID_TO_DTS
        edl_valid_from_datetime_utc (str): column name to capture start datetime UTC of a record. Example- EDL_VALID_FROM_DTS_UTC
        edl_valid_to_datetime_utc (str): column name to capture end datetime UTC of a record. Example- EDL_VALID_TO_DTS_UTC        
        edl_active_flag (str): column name that holds 'Y' if the record is active and 'N' if the record is inactive. Example- EDL_ACTIVE_FLG
        edl_deleted_flag (str): column name that holds 'Y' if the record is active and 'N' if the record is inactive. Example- EDL_DELETED_FLG
        merge_schema (bool): If True, (changed)schema will be merged. If False, schema will not be merged.

    Returns:
        dict: 3 dictionaries with the prepared insert, update and delete columns.
    """

    utils_logger.info("Insert, Update and Delete columns preparation for historization")

    # Validate parameters  
    if load_type.upper() not in ['SCD1', 'SCD2']:
        raise ValueError("Invalid load_type. Expected 'SCD1' or 'SCD2'.")

    try:
        date.fromisoformat(processing_date)
        processing_date = processing_date + " 00:00:00.000"
    except ValueError:
        raise ValueError("Incorrect data format, should be YYYY-MM-DD")

    # Validate Kwargs
    valid_kwargs = validate_input_kwargs(**kwargs)
 
    #Validate if partition, edl_act_datetime_value columns exist in source dataframe
    ValidationUtils.cols_present_check(source_df.columns,[partition_column, 
                                                          kwargs['edl_act_datetime_value']])

    #EDL columns to ignore during historization
    edl_key_name_list = [ 'edl_load_datetime'
                        , 'edl_valid_from_datetime'
                        , 'edl_valid_from_datetime_utc'
                        , 'edl_valid_to_datetime'
                        , 'edl_valid_to_datetime_utc'
                        , 'edl_act_datetime'
                        , 'edl_update_datetime'
                        , 'edl_active_flag'
                        , 'edl_deleted_flag'                   
                        ]
 
    cur_datetime\
    , start_datetime\
    , close_datetime\
    , open_datetime\
    , start_datetime_utc\
    , close_datetime_utc\
    , edl_act_datetime_utc_value = initialize_timestamps(processing_date
                                                        , source_df
                                                        , **valid_kwargs)

    insert_col = prepare_insert_columns(source_df, 
                                        load_type, 
                                        partition_name, 
                                        partition_column, 
                                        edl_key_name_list, 
                                        cur_datetime, 
                                        start_datetime, 
                                        open_datetime, 
                                        start_datetime_utc, 
                                        edl_act_datetime_utc_value,
                                        **valid_kwargs)

    update_col = prepare_update_columns(edl_key_name_list, 
                                        cur_datetime, 
                                        source_df, 
                                        load_type, 
                                        close_datetime, 
                                        close_datetime_utc,
                                        **valid_kwargs)

    delete_col = prepare_delete_columns(cur_datetime, 
                                        close_datetime, 
                                        close_datetime_utc,
                                        **valid_kwargs)

    utils_logger.info("Insert, Update and Delete columns prepared for historization")
    return insert_col, update_col, delete_col

def initialize_timestamps(processing_date, source_df, **kwargs):
    """
    Initialize various timestamp values based on the provided processing date and additional parameters.

    Parameters:
        processing_date (str): The processing date in 'yyyy-MM-dd HH:mm:ss.SSS' format.
        source_df (DataFrame): The source DataFrame containing the data.

    Keyword Arguments:
        timezone (str): The timezone to convert timestamps to UTC.
        edl_act_datetime_value (str): The column name in source_df for the EDL actual datetime value.

    Returns:
        tuple: A tuple containing the following timestamp values:
            - cur_datetime (Timestamp): The current timestamp.
            - start_datetime (Timestamp): The start timestamp based on the processing date.
            - close_datetime (Timestamp): The close timestamp, one second before the start timestamp.
            - open_datetime (Timestamp): A fixed future timestamp ('9999-12-31 00:00:00.000').
            - start_datetime_utc (Timestamp): The start timestamp converted to UTC (if timezone is provided).
            - close_datetime_utc (Timestamp): The close timestamp converted to UTC (if timezone is provided).
            - edl_act_datetime_utc_value (Timestamp): The EDL actual datetime value converted to UTC (if timezone is provided).
    """
    cur_datetime = current_timestamp()
    open_datetime = to_timestamp(lit('9999-12-31 00:00:00.000'), 'yyyy-MM-dd HH:mm:ss.SSS')
    start_datetime = to_timestamp(lit(processing_date), 'yyyy-MM-dd HH:mm:ss.SSS')
    close_datetime = to_timestamp(lit(processing_date), 'yyyy-MM-dd HH:mm:ss.SSS') - expr('INTERVAL 1 SECOND')

    if kwargs.get('timezone'):
        start_datetime_utc = to_utc_timestamp(start_datetime, kwargs['timezone'])
        close_datetime_utc = to_utc_timestamp(close_datetime, kwargs['timezone'])
        edl_act_datetime_utc_value = to_utc_timestamp(to_timestamp(source_df[kwargs['edl_act_datetime_value']]), kwargs['timezone'])
    else:
        start_datetime_utc = start_datetime
        close_datetime_utc = close_datetime
        edl_act_datetime_utc_value = source_df[kwargs['edl_act_datetime_value']]
        
    utils_logger.info("helper function initialize_timestamps completed successfully")
    return cur_datetime, start_datetime, close_datetime, open_datetime, start_datetime_utc, close_datetime_utc, edl_act_datetime_utc_value

def prepare_insert_columns(source_df, load_type, partition_name, partition_column, edl_key_name_list,\
                                 cur_datetime, start_datetime, open_datetime, start_datetime_utc,\
                                 edl_act_datetime_utc_value, **kwargs):
    """
    Prepare a dictionary of columns for inserting records into the target table.

    Parameters:
        source_df (DataFrame): The source DataFrame containing the data.
        load_type (str): The type of load operation (e.g., 'SCD2').
        partition_name (str): The name of the partition column in the target table.
        partition_column (str): The column in source_df used for partitioning.
        edl_key_name_list (list): A list of key column names to exclude from the insert columns.
        cur_datetime (Timestamp): The current timestamp.
        start_datetime (Timestamp): The start timestamp.
        open_datetime (Timestamp): A fixed future timestamp ('9999-12-31 00:00:00.000').
        start_datetime_utc (Timestamp): The start timestamp converted to UTC.
        edl_act_datetime_utc_value (Timestamp): The actual datetime value converted to UTC.
    
    Keyword Arguments:
        edl_load_datetime (str): The column name for the load datetime.
        edl_act_datetime (str): The column name for the actual datetime.
        edl_act_datetime_value (str): The column name in source_df for the actual datetime value.
        edl_act_datetime_utc (str): The column name for the actual datetime in UTC.
        edl_update_datetime (str): The column name for the update datetime.
        edl_valid_from_datetime (str, optional): The column name for the valid-from datetime (for SCD2).
        edl_valid_from_datetime_utc (str, optional): The column name for the valid-from datetime in UTC (for SCD2).
        edl_valid_to_datetime (str, optional): The column name for the valid-to datetime (for SCD2).
        edl_valid_to_datetime_utc (str, optional): The column name for the valid-to datetime in UTC (for SCD2).
        edl_active_flag (str, optional): The column name for the active flag (for SCD2).
        edl_deleted_flag (str, optional): The column name for the deleted flag (for SCD2).

    Returns:
        dict: A dictionary with the columns to be inserted into the target table.
    """
    insert_col = {}

    for c in source_df.columns:
        if c not in edl_key_name_list:
            insert_col[c] = c

    insert_col.update({
        partition_name: source_df[partition_column],
        kwargs['edl_load_datetime']: cur_datetime,
        kwargs['edl_act_datetime']: source_df[kwargs['edl_act_datetime_value']],
        kwargs['edl_act_datetime_utc']: edl_act_datetime_utc_value,
        kwargs['edl_update_datetime']: cur_datetime
        #kwargs['edl_change_type']: lit('Insert'),
    })

    if load_type.upper() == 'SCD2':
        insert_col.update({
            kwargs['edl_valid_from_datetime']: start_datetime,
            kwargs['edl_valid_from_datetime_utc']: start_datetime_utc,
            kwargs['edl_valid_to_datetime']: open_datetime,
            kwargs['edl_valid_to_datetime_utc']: open_datetime,
            kwargs['edl_active_flag']: lit('Y'),
            kwargs['edl_deleted_flag']: lit('N')
        })

    utils_logger.info("insert column preparation completed successfully")
    return insert_col

def prepare_update_columns(edl_key_name_list, cur_datetime, source_df, load_type, close_datetime, close_datetime_utc, **kwargs):
    """
    Prepare a dictionary of columns for updating records in the target table.

    Parameters:
        edl_key_name_list (list): A list of key column names to exclude from the update columns.
        cur_datetime (Timestamp): The current timestamp.
        source_df (DataFrame): The source DataFrame containing the data.
        load_type (str): The type of load operation (e.g., 'SCD1', 'SCD2').
        close_datetime (Timestamp): The close timestamp.
        close_datetime_utc (Timestamp): The close timestamp in UTC.
    
    Keyword Arguments:
        edl_update_datetime (str): The column name for the update datetime.
        edl_valid_to_datetime (str, optional): The column name for the valid-to datetime (for SCD2).
        edl_valid_to_datetime_utc (str, optional): The column name for the valid-to datetime in UTC (for SCD2).
        edl_active_flag (str, optional): The column name for the active flag (for SCD2).
        edl_deleted_flag (str, optional): The column name for the deleted flag (for SCD2).

    Returns:
        dict: A dictionary with the columns to be updated in the target table.
    """
    update_col = {
        kwargs['edl_update_datetime']: cur_datetime,
        #kwargs['edl_change_type']: lit('Update')
    }

    if load_type.upper() == 'SCD1':
        for c in source_df.columns:
            if c not in edl_key_name_list:
                update_col[c] = f'src.{c}'
    elif load_type.upper() == 'SCD2':
        update_col.update({
            kwargs['edl_valid_to_datetime']: close_datetime,
            kwargs['edl_valid_to_datetime_utc']: close_datetime_utc,
            kwargs['edl_active_flag']: lit('N'),
            kwargs['edl_deleted_flag']: lit('N')
        })

    utils_logger.info("update column preparation completed successfully")
    return update_col

def prepare_delete_columns(cur_datetime, close_datetime, close_datetime_utc, **kwargs):
    """
    Prepare a dictionary of columns for marking records as deleted.

    Parameters:
        cur_datetime (Timestamp): The current timestamp.
        close_datetime (Timestamp): The close timestamp.
        close_datetime_utc (Timestamp): The close timestamp in UTC.
    
    Keyword Arguments:
        edl_update_datetime (str): The column name for the update datetime.
        edl_valid_to_datetime (str): The column name for the valid-to datetime.
        edl_valid_to_datetime_utc (str): The column name for the valid-to datetime in UTC.
        edl_active_flag (str): The column name for the active flag(for SCD2).
        edl_deleted_flag (str): The column name for the deleted flag(for SCD2).

    Returns:
        dict: A dictionary with the columns to be marked as deleted into the target table.
    """
    delete_col = {
        kwargs['edl_update_datetime']: cur_datetime,
        #kwargs['edl_change_type']: lit("Delete"),
        kwargs['edl_valid_to_datetime']: close_datetime,
        kwargs['edl_valid_to_datetime_utc']: close_datetime_utc,
        kwargs['edl_active_flag']: lit('N'),
        kwargs['edl_deleted_flag']: lit('Y')
    }
    utils_logger.info("delete column preparation completed successfully")
    return delete_col

def prepare_scd2_history_columns(source_df, partition_name, partition_column, scd2_history_column , **kwargs):
    """
    Prepares columns for SCD2 bulk history load.

    Args:
        source_df (DataFrame): The source DataFrame containing the data.
        partition_name (str): The name of the partition column.
        partition_column (str): The column name used for partitioning.
        scd2_history_column (str): The column name containing the SCD2 history timestamp.

    Keyword Arguments:
        edl_act_datetime_value (str): Column name for the actual datetime value.
        edl_load_datetime (str): Column name for the load datetime.
        edl_act_datetime (str): Column name for the actual datetime.
        edl_act_datetime_utc (str): Column name for the actual datetime in UTC.
        edl_update_datetime (str): Column name for the update datetime.
        edl_valid_from_datetime (str): Column name for the valid from datetime.
        edl_valid_from_datetime_utc (str): Column name for the valid from datetime in UTC.
        edl_valid_to_datetime (str): Column name for the valid to datetime.
        edl_valid_to_datetime_utc (str): Column name for the valid to datetime in UTC.
        edl_active_flag (str): Column name for the active flag.
        edl_deleted_flag (str): Column name for the deleted flag.
        timezone (str, optional): Timezone for converting timestamps to UTC.

    Returns:
        dict: A dictionary containing the prepared columns for SCD2 history load.
    """

    #Validate if partition, edl_act_datetime_value, edl_active_flag,  columns exist in source dataframe
    ValidationUtils.cols_present_check(source_df.columns,[partition_column, scd2_history_column,
                                                          kwargs['edl_act_datetime_value'], 'edl_active_flag', 'edl_valid_to_datetime'])
    cur_datetime = current_timestamp()
    start_datetime = to_timestamp(source_df[scd2_history_column])
    close_datetime = to_timestamp(source_df['edl_valid_to_datetime'])
    act_datetime = to_timestamp(source_df[kwargs['edl_act_datetime_value']])
    
    if kwargs.get('timezone'):
        start_datetime_utc_value = to_utc_timestamp(start_datetime, kwargs['timezone'])
        close_datetime_utc_value = to_utc_timestamp(close_datetime, kwargs['timezone'])
        act_datetime_utc_value = to_utc_timestamp(act_datetime, kwargs['timezone'])
    else:
        start_datetime_utc_value = start_datetime
        close_datetime_utc_value = close_datetime
        act_datetime_utc_value = act_datetime

    insert_col = {
        partition_name: source_df[partition_column],
        kwargs['edl_load_datetime']: cur_datetime,
        kwargs['edl_act_datetime']: act_datetime,
        kwargs['edl_act_datetime_utc']: act_datetime_utc_value,
        kwargs['edl_update_datetime']: cur_datetime,
        kwargs['edl_valid_from_datetime']: start_datetime,
        kwargs['edl_valid_from_datetime_utc']: start_datetime_utc_value,
        kwargs['edl_valid_to_datetime']: close_datetime,
        kwargs['edl_valid_to_datetime_utc']: close_datetime_utc_value,
        kwargs['edl_active_flag']: source_df['edl_active_flag'],
        kwargs['edl_deleted_flag']:  lit('N')
    }

    utils_logger.info("column preparation for scd2 Bulk history load completed successfully")
    return insert_col

def generate_nonkey_column(
        input_df: DataFrame,
        primary_keys: list,
        exclude_columns: list) -> list:
    """
    Generate NonKey columns of a dataframe

    Args:
        input_df (DataFrame): Source dataframe
        primary_keys (list): List of Primary Key columns
        exclude_columns (list): List of column names those should not be part of Non_key

    Returns:
        List: A list of non key columns

    Raises:
        ValueError(f"Wrong argument: {e.__str__()}")
        ValueError(f"NonKey generation failed: {e.__str__()}")

    Usage example:
    --------------
        >>> from rabobank_edp_dbr_utils.transformation import generate_nonkey_column
        >>> data = [(1, "A", "2021-01-01", "X"), (2, "B", "2021-01-02", "Y"), (3, "C", "2021-01-03", "Z")]
        >>> schema = ["id", "value", "date", "extra"]
        >>> df = spark.createDataFrame(data, schema)
        >>> primary_keys = ["id"]
        >>> exclude_columns = ["extra"]
        >>> non_key_columns = generate_nonkey_column(df, primary_keys, exclude_columns)
        >>> print(non_key_columns)
        ['value', 'date']
    """

    utils_logger.info(f"Generating NonKeys from primary_key: {primary_keys} and exclude_column: {exclude_columns}")

    try:
        if primary_keys and exclude_columns:
            #column check for primary keys and exclude_columns
            ValidationUtils.cols_present_check(input_df.columns,primary_keys)
            ValidationUtils.cols_present_check(input_df.columns,exclude_columns)
        else:
            raise ValueError(f"Wrong argument")
        columns =  input_df.columns
        [columns.remove(col) for col in primary_keys+exclude_columns]

    except Exception as e:
        utils_logger.error(f"NonKey generation failed: {e.__str__()}")
        raise ValueError(f"NonKey generation failed from columns primary_key: {primary_keys} and exclude_column: {exclude_columns}")
    
    utils_logger.info("NonKey generation is successful")
    return columns

def create_content_hash(
    df:DataFrame,
    included_column_list: list,
    hash_column_name="content_hash",
    delimiter="|") -> DataFrame:
    """
    This function is used to generate hash value of all the columns in dataset which are passed as input column list. 
    Args:
        df : dataframe to have hashing
        included_column_list (list): column list that should be hashed.
        hash_column_name:Name of the hashed column(optional).
        delimiter:Delimiter for input columns(optional)
    """
    utils_logger.info(f"Creating hash of the columns: {included_column_list}")
    
    # Validate Dataframe 
    if DataFrame is None:
      raise ValueError("Input DataFrame is not defined.")
    # Validate included_column_list is a list
    if not isinstance(included_column_list, list):
      raise TypeError("The included_column_list must be a list")
    
    # Check for duplicate input column in dataframe
    if len(included_column_list) != len(set(included_column_list)):
      raise ValueError(f'There are duplicate columns in the input for. Please remove duplicates')
    
    # Check if the input data is a list and not empty
    ValidationUtils.cols_present_check(df.columns,included_column_list)

    # Creating hash of the required columns                            
    df_hashed = df.withColumn(hash_column_name,sha2(concat_ws(delimiter,*included_column_list),256))

    utils_logger.info(f"Hash column: {hash_column_name} created successfully")
    return(df_hashed)

def insert_columns_if_not_exists(source_df: DataFrame, insert_col: dict) -> DataFrame:
    """
    Insert columns to the source DataFrame if they do not exist.

    Args:
        source_df (DataFrame): The source DataFrame.
        insert_col (dict): A dictionary containing the columns to be inserted. 
            The key is the column name, and the value is the column value.

    Returns:
        DataFrame: The source DataFrame with the new columns inserted.
    """

    for c in insert_col.keys():
        if c not in source_df.columns:
            source_df = source_df.withColumn(c, insert_col[c])

    return source_df

def get_order_by_cols(sorting_cols: dict):
    """
    Generates a list of order-by column expressions based on the provided sorting order.

    Parameters:
        sorting_cols (dict): (key: column name, value: sorting order 'asc' or 'desc')A dictionary where keys are column names and values are sorting orders ('asc' or 'desc').

    Returns:
        list: A list of order-by column expressions.

    Raises:
        Exception: If a sorting order other than 'asc' or 'desc' is provided.

    Usage example:
    --------------
        >>> sorting_cols = {'name': 'asc', 'age': 'desc'}
        >>> order_by_cols = get_order_by_cols(sorting_cols)
        order_by_cols will contain [asc_nulls_last('name'), desc('age')]
    """

    order_by_cols=[]
    for colname_key in sorting_cols:
        if sorting_cols[colname_key] == 'desc':
           order_by_cols.append(desc(colname_key))
        elif sorting_cols[colname_key] == "asc":
            order_by_cols.append(asc_nulls_last(colname_key))
        else:
            utils_logger.error("Invalid sorting order. It should be desc or asc")
            raise Exception("Invalid sorting order. It should be desc or asc")

    return order_by_cols